/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.pszymczyk.kafka;

import com.pszymczyk.kafka.api.KafkaTransactionManagerBuilder;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.function.Consumer;

import static java.util.concurrent.TimeUnit.SECONDS;
import static org.awaitility.Awaitility.await;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.mockito.Mockito.*;

class KafkaTransactionManagerTest {

    private static final TopicPartition outputTopic = new TopicPartition("output", 0);
    private static final TopicPartition inputTopic = new TopicPartition("input", 0);

    private String groupId;

    @BeforeAll
    public static void setupKafka() {
        KafkaContainerStarter.start();
    }

    @BeforeEach
    public void setup() {
        groupId = UUID.randomUUID().toString().substring(0, 7);
    }

    @Test
    void shouldRetryOnce() {
        //given
        var kafkaProducer = kafkaProducer();
        var consumer = kafkaConsumer(groupId);
        consumer.assign(List.of(inputTopic));
        Consumer<Exception> exceptionHandler = mock(Consumer.class);
        Consumer<ConsumerRecord<String, String>> retryExhaustedMock = mock(Consumer.class);
        final boolean[] thrown = {false};
        var kafkaTransactionManager = KafkaTransactionManagerBuilder.<String, String, String, String>newKafkaTransactionManager()
                .withTransactionPerRecord(consumerRecord -> {
                    if (thrown[0]) {
                        return List.of(new ProducerRecord<>(outputTopic.topic(), "pong"));
                    } else {
                        thrown[0] = true;
                        throw new RuntimeException("Upssss...");
                    }
                })
                .withRetries(3)
                .withConsumerGroupMetadata(new ConsumerGroupMetadata("some-group"))
                .withExceptionHandler(exceptionHandler)
                .withRetriesExhaustedHandler(retryExhaustedMock)
                .build(kafkaProducer);
        ConsumerRecords<String, String> consumerRecords = await()
                .atMost(3, SECONDS)
                .until(() -> consumer.poll(Duration.ofMillis(100)), consumerRecords1 -> !consumerRecords1.isEmpty());

        //when
        kafkaTransactionManager.executeInTransaction(consumerRecords);

        //then
        verify(exceptionHandler, times(1)).accept(any());
        verifyNoInteractions(retryExhaustedMock);
        consumer.assign(List.of(outputTopic));
        consumer.seekToBeginning(List.of(outputTopic));
        ConsumerRecords<String, String> until = await().atMost(3, SECONDS)
                .until(() -> consumer.poll(Duration.ofMillis(100)),
                        consumerRecords1 -> !consumerRecords1.isEmpty());
        List<ConsumerRecord<String, String>> records = until.records(outputTopic);
        assertEquals("pong", records.get(records.size() - 1).value());
    }

    @Test
    void shouldRetry() {
        //given
        var kafkaProducer = kafkaProducer();
        var consumer = kafkaConsumer(groupId);
        consumer.assign(List.of(inputTopic));
        Consumer<Exception> exceptionHandler = mock(Consumer.class);
        Consumer<ConsumerRecord<String, String>> retryExhaustedMock = mock(Consumer.class);
        var kafkaTransactionManager = KafkaTransactionManagerBuilder.<String, String, String, String>newKafkaTransactionManager()
                .withTransactionPerRecord(consumerRecord -> {
                    throw new RuntimeException("Upssss...");
                })
                .withRetries(3)
                .withConsumerGroupMetadata(new ConsumerGroupMetadata("some-group"))
                .withExceptionHandler(exceptionHandler)
                .withRetriesExhaustedHandler(retryExhaustedMock)
                .build(kafkaProducer);
        ConsumerRecords<String, String> consumerRecords = await()
                .atMost(3, SECONDS)
                .until(() -> consumer.poll(Duration.ofMillis(100)), cR -> !cR.isEmpty());

        //when
        kafkaTransactionManager.executeInTransaction(consumerRecords);

        //then
        verify(exceptionHandler, times(4)).accept(any());
        verify(retryExhaustedMock).accept(any());
    }

    @Test
    void shouldPerformTransaction() {
        //given
        var kafkaProducer = kafkaProducer();
        var consumer = kafkaConsumer(groupId);
        consumer.assign(List.of(inputTopic));
        var kafkaTransactionManager = KafkaTransactionManagerBuilder.<String, String, String, String>newKafkaTransactionManager()
                .withTransactionPerRecord(consumerRecord -> List.of(new ProducerRecord<>(outputTopic.topic(), "pong")))
                .withRetries(3)
                .withConsumerGroupMetadata(new ConsumerGroupMetadata("some-group"))
                .build(kafkaProducer);
        ConsumerRecords<String, String> consumerRecords = await()
                .atMost(3, SECONDS)
                .until(() -> consumer.poll(Duration.ofMillis(100)), cR -> !cR.isEmpty());

        //when
        kafkaTransactionManager.executeInTransaction(consumerRecords);

        //then
        consumer.assign(List.of(outputTopic));
        consumer.seekToBeginning(List.of(outputTopic));
        ConsumerRecords<String, String> output = await()
                .atMost(3, SECONDS)
                .until(() -> consumer.poll(Duration.ofMillis(100)), cR -> !cR.isEmpty());
        var records = output.records(outputTopic);
        assertEquals("pong", records.get(records.size() - 1).value());
    }


    protected KafkaConsumer<String, String> kafkaConsumer(String groupId) {
        return new KafkaConsumer<>(
                Map.of(
                        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaContainerStarter.getBootstrapServers(),
                        ConsumerConfig.GROUP_ID_CONFIG, groupId,
                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest",
                        ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class,
                        ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class,
                        ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 3,
                        ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false)
        );
    }

    protected static KafkaProducer<String, String> kafkaProducer() {
        return new KafkaProducer<>(Map.of(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaContainerStarter.getBootstrapServers(),
                ProducerConfig.TRANSACTIONAL_ID_CONFIG, "trala-1",
                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class,
                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class));
    }
}
